{"cells":[{"cell_type":"markdown","source":["# Distributed Deep Learning training con Horovod y TensorFlow\n## Sobre Horovod y Databricks\n\nHorovodRunner es una API para ejecutar workloads de deep learning distribuidos en Databricks utilizando el framework Horovod (proyecto iniciado por Uber). Al integrar Horovod con el modo barrier de Spark, Databricks es capaz de ofrecer entrenamiento de modelos de machine learning utilizando deep learning. HorovodRunner permite ejecutar un metodo en Python que especifica una rutina de entrenamiento de un modelo que incluye hooks para Horovod. \n\n<img src=\"https://docs.databricks.com/_images/horovod-runner.png\" />"],"metadata":{}},{"cell_type":"code","source":["import warnings\nwarnings.filterwarnings(\"ignore\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["## Demo: MNIST dataset\n\nThe MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning.\n\n<img src=\"https://greydanus.github.io/assets/subspace-nn/mnist.png\" />"],"metadata":{}},{"cell_type":"markdown","source":["### Utilizando Deep Learning para resolver el problema\n#### Preparando un directorio para checkpointing (Deep Learning Storage)\nTensorFlow utiliza checkpoints para almacenar el estado del entrenamiento del modelo. Crearemos un directorio dentro de DBFS para este proposito. Durante un entrenamiento distribuido y para evitar que todos los nodos modifiquen este estado, el checkpoint será utilizado por solo uno de los nodos"],"metadata":{}},{"cell_type":"code","source":["import os\nimport time\n\ncheckpoint_dir = '/dbfs/ml/MNISTDemo/train/{}/'.format(time.time())\nos.makedirs(checkpoint_dir)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["Creamos una función get_dataset que nos permite obtener un dataset de MNIST. Las imagenes las obtenemos de los datasets standard de Keras"],"metadata":{}},{"cell_type":"code","source":["from tensorflow import keras\n\ndef get_dataset(num_classes, rank=0, size=1):\n  (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data('MNIST-data-%d' % rank)\n  x_train = x_train[rank::size]\n  y_train = y_train[rank::size]\n  x_test = x_test[rank::size]\n  y_test = y_test[rank::size]\n  x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n  x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n  x_train = x_train.astype('float32')\n  x_test = x_test.astype('float32')\n  x_train /= 255\n  x_test /= 255\n  y_train = keras.utils.to_categorical(y_train, num_classes)\n  y_test = keras.utils.to_categorical(y_test, num_classes)\n  \n  return (x_train, y_train), (x_test, y_test)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["## Constuimos el modelo utilizando Keras API\n\nCrearemos un modelo basado en CNN utilizando la API secuencial de Keras con 2 capas de convolucion, dropout para agregar regularizaton y finalmente un fully connected layer"],"metadata":{}},{"cell_type":"code","source":["from tensorflow.keras import models\nfrom tensorflow.keras import layers\n\ndef get_model(num_classes):\n  model = models.Sequential()\n  model.add(layers.Conv2D(32, kernel_size=(3, 3),\n                   activation='relu',\n                   input_shape=(28, 28, 1)))\n  model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n  model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n  model.add(layers.Dropout(0.25))\n  model.add(layers.Flatten())\n  model.add(layers.Dense(128, activation='relu'))\n  model.add(layers.Dropout(0.5))\n  model.add(layers.Dense(num_classes, activation='softmax'))\n  \n  return model"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["Inspeccionemos como luce este modelo"],"metadata":{}},{"cell_type":"code","source":["model = get_model(10)\nmodel.summary()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Model: &#34;sequential_3&#34;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_6 (Conv2D)            (None, 26, 26, 32)        320       \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 24, 24, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 12, 12, 64)        0         \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 12, 12, 64)        0         \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 9216)              0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 128)               1179776   \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 10)                1290      \n=================================================================\nTotal params: 1,199,882\nTrainable params: 1,199,882\nNon-trainable params: 0\n_________________________________________________________________\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["#### Rutina de entrenamiento tradicional\n\nNormalmente, en TensorFlow armariamos una rutina de entrenamiento como la siguiente, donde especificariamos el optimizer a utilizar, compilariamos el modelo e iniciariamos el entrenamiento utilizando el metodo fit."],"metadata":{}},{"cell_type":"code","source":["num_classes = 10"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["def train(learning_rate=1.0, batch_size = 128, epochs = 5):\n  (x_train, y_train), (x_test, y_test) = get_dataset(num_classes)\n  model = get_model(num_classes)\n\n  optimizer = keras.optimizers.Adadelta(lr=learning_rate)\n\n  model.compile(optimizer=optimizer,\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])\n\n  training_history = model.fit(x_train, y_train,\n                        batch_size=batch_size,\n                        epochs=epochs,\n                        verbose=2,\n                        validation_data=(x_test, y_test))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["#### Runtina de entrenamiento distribuida con Horovod\n\nPara utilizar horovod necesitamos realizar algunas modificaciones a nuestra rutina de entrenamiento. Principalmente cambiaremos:\n\n\n- Inicializar Horovod\n- Configurar la session de TensorFlow para que utilice tantos procesos como nodos tenemos disponibles\n- Configuramos un optimizer especifico para Horovod como un wrapper del optimizer utilizado en nuestro modelo\n- Nos aseguramos que la inicializacion del modelo se realiza consistentemente en todos los nodos\n- Guardamos el checkpoint del entrenamiento en el directorio creado anteriormente, pero solo en el nodo driver"],"metadata":{}},{"cell_type":"code","source":["# Horovod: Import the relevant submodule\nimport horovod.tensorflow.keras as hvd\n\nfrom tensorflow.keras import backend as K\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\ndef train_hvd(learning_rate=1.0, batch_size=512, epochs=5):\n  # Horovod: initialize Horovod.\n  hvd.init()\n\n  # Horovod: pin GPU to be used to process local rank (one GPU per process)\n  config = tf.ConfigProto()\n  config.gpu_options.allow_growth = True\n  config.gpu_options.visible_device_list = str(hvd.local_rank())\n  K.set_session(tf.Session(config=config))\n\n  (x_train, y_train), (x_test, y_test) = get_dataset(num_classes, hvd.rank(), hvd.size())\n  model = get_model(num_classes)\n\n  #hvd.size() returns the numer of GPUs\n  optimizer = keras.optimizers.Adadelta(lr=learning_rate * hvd.size())\n\n  # Add Horovod Distributed Optimizer.\n  optimizer = hvd.DistributedOptimizer(optimizer)\n\n  model.compile(optimizer=optimizer,\n                loss='categorical_crossentropy',\n                metrics=['accuracy'])\n\n  callbacks = [\n      # Horovod: broadcast initial variable states from rank 0 to all other processes.\n      # This is necessary to ensure consistent initialization of all workers when\n      # training is started with random weights or restored from a checkpoint.\n      hvd.callbacks.BroadcastGlobalVariablesCallback(0),\n  ]\n\n  # Horovod: save checkpoints only on worker 0 to prevent other workers from corrupting them.\n  if hvd.rank() == 0:\n      callbacks.append(keras.callbacks.ModelCheckpoint(checkpoint_dir + '/checkpoint-{epoch}.ckpt', save_weights_only = True))\n\n  train_history = model.fit(x_train, y_train,\n              batch_size=batch_size,\n              callbacks=callbacks,\n              epochs=epochs,\n              verbose=2,\n              validation_data=(x_test, y_test))\n  \n  \n  if hvd.rank() == 0:\n    # Plot training & validation accuracy values\n    plt.plot(train_history.history['acc'])\n    plt.plot(train_history.history['val_acc'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n    \n    plt.savefig(checkpoint_dir + '/train_acc.png')\n\n    # Plot training & validation loss values\n    plt.plot(train_history.history['loss'])\n    plt.plot(train_history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Test'], loc='upper left')\n    plt.show()\n    \n    plt.savefig(checkpoint_dir + '/train_loss.png')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"markdown","source":["#### Iniciamos el entrenamiento\n\nLa siguiente porción de código inicia la rutina de entrenamiento utilizando un Horovod Runner. Como argumentos especificamos los parametros que deben ser enviados a la función que indicamos anteriormente"],"metadata":{}},{"cell_type":"code","source":["from sparkdl import HorovodRunner\n\nhr = HorovodRunner(np=2)\nmodel = hr.run(train_hvd, learning_rate=0.1, batch_size=512, epochs=5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">The global names read or written to by the pickled function are {&#39;K&#39;, &#39;num_classes&#39;, &#39;str&#39;, &#39;keras&#39;, &#39;hvd&#39;, &#39;get_model&#39;, &#39;tf&#39;, &#39;checkpoint_dir&#39;, &#39;plt&#39;, &#39;get_dataset&#39;}.\nThe pickled object size is 3982 bytes.\n\n### How to enable Horovod Timeline? ###\nHorovodRunner has the ability to record the timeline of its activity with Horovod  Timeline. To\nrecord a Horovod Timeline, set the `HOROVOD_TIMELINE` environment variable  to the location of the\ntimeline file to be created. You can then open the timeline file  using the chrome://tracing\nfacility of the Chrome browser.\n\nStart training.\n[1,1]&lt;stderr&gt;:Using TensorFlow backend.\n[1,0]&lt;stderr&gt;:Using TensorFlow backend.\n[1,1]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:117: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n[1,1]&lt;stderr&gt;:\n[1,1]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:143: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n[1,1]&lt;stderr&gt;:\n[1,0]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:117: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n[1,0]&lt;stderr&gt;:\n[1,0]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/horovod/tensorflow/__init__.py:143: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n[1,0]&lt;stderr&gt;:\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:57.765176: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:57.765562: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:57.773319: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2596990000 Hz\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:57.773741: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2596990000 Hz\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:57.774833: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5632e0cc3050 executing computations on platform Host. Devices:\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:57.774864: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:57.775393: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561a4fbeffe0 executing computations on platform Host. Devices:\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:57.775429: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:57.778330: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:57.778331: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:58.023622: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5632e0db2d50 executing computations on platform CUDA. Devices:\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:58.023669: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:58.023680: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x561a4fcdfce0 executing computations on platform CUDA. Devices:\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:58.023711: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:58.025897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n[1,0]&lt;stderr&gt;:name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n[1,0]&lt;stderr&gt;:pciBusID: 72e6:00:00.0\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:58.025918: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n[1,1]&lt;stderr&gt;:name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n[1,1]&lt;stderr&gt;:pciBusID: 90b7:00:00.0\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:58.025979: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:58.025984: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:58.027370: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:58.027372: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:58.028587: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:58.028586: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:58.028913: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:58.028935: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:58.030453: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:58.030453: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:58.031662: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:58.031669: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:58.035314: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:58.035313: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:58.038436: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 1\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:58.038476: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:58.038492: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:58.038533: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:58.093053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:58.093125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      1 \n[1,1]&lt;stderr&gt;:2019-12-03 13:30:58.093139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   N \n[1,0]&lt;stderr&gt;:2019-12-03 13:30:58.094501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:58.094545: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n[1,0]&lt;stderr&gt;:2019-12-03 13:30:58.094558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n[1,1]&lt;stderr&gt;:2019-12-03 13:30:58.095973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10798 MB memory) -&gt; physical GPU (device: 1, name: Tesla K80, pci bus id: 90b7:00:00.0, compute capability: 3.7)\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:58.097572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10798 MB memory) -&gt; physical GPU (device: 0, name: Tesla K80, pci bus id: 72e6:00:00.0, compute capability: 3.7)\n[1,1]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n[1,1]&lt;stderr&gt;:Instructions for updating:\n[1,1]&lt;stderr&gt;:Call initializer instance with the dtype argument instead of passing it to the constructor\n[1,1]&lt;stderr&gt;:From /databricks/python/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n[1,1]&lt;stderr&gt;:Instructions for updating:\n[1,1]&lt;stderr&gt;:Call initializer instance with the dtype argument instead of passing it to the constructor\n[1,0]&lt;stderr&gt;:WARNING:tensorflow:From /databricks/python/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n[1,0]&lt;stderr&gt;:Instructions for updating:\n[1,0]&lt;stderr&gt;:Call initializer instance with the dtype argument instead of passing it to the constructor\n[1,0]&lt;stderr&gt;:From /databricks/python/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n[1,0]&lt;stderr&gt;:Instructions for updating:\n[1,0]&lt;stderr&gt;:Call initializer instance with the dtype argument instead of passing it to the constructor\n[1,1]&lt;stderr&gt;:/databricks/python/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py:61: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working\n[1,1]&lt;stderr&gt;:  elif not isinstance(value, collections.Sized):\n[1,0]&lt;stderr&gt;:/databricks/python/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py:61: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working\n[1,0]&lt;stderr&gt;:  elif not isinstance(value, collections.Sized):\n[1,1]&lt;stdout&gt;:Train on 30000 samples, validate on 5000 samples\n[1,0]&lt;stdout&gt;:Train on 30000 samples, validate on 5000 samples\n[1,1]&lt;stdout&gt;:Epoch 1/5\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:59.548712: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n[1,0]&lt;stdout&gt;:Epoch 1/5\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:59.678642: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n[1,1]&lt;stderr&gt;:2019-12-03 13:30:59.702750: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n[1,0]&lt;stderr&gt;:2019-12-03 13:30:59.838610: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n[1,0]&lt;stdout&gt;:1203-050510-sloop247-10-139-64-5:8171:8181 [0] NCCL INFO NET/Socket : Using [0]eth0:10.139.64.5&lt;0&gt;\n[1,0]&lt;stdout&gt;:1203-050510-sloop247-10-139-64-5:8171:8181 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\n[1,0]&lt;stdout&gt;:\n[1,0]&lt;stdout&gt;:1203-050510-sloop247-10-139-64-5:8171:8181 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\n[1,0]&lt;stdout&gt;:NCCL version 2.4.7+cuda10.0\n[1,1]&lt;stdout&gt;:1203-050510-sloop247-10-139-64-5:8172:8179 [1] NCCL INFO NET/Socket : Using [0]eth0:10.139.64.5&lt;0&gt;\n[1,1]&lt;stdout&gt;:1203-050510-sloop247-10-139-64-5:8172:8179 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so).\n[1,1]&lt;stdout&gt;:\n[1,1]&lt;stdout&gt;:1203-050510-sloop247-10-139-64-5:8172:8179 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]\n[1,0]&lt;stdout&gt;:1203-050510-sloop247-10-139-64-5:8171:8181 [0] NCCL INFO Setting affinity for GPU 0 to 0fff\n[1,1]&lt;stdout&gt;:1203-050510-sloop247-10-139-64-5:8172:8179 [1] NCCL INFO Setting affinity for GPU 1 to 0fff\n[1,0]&lt;stdout&gt;:1203-050510-sloop247-10-139-64-5:8171:8181 [0] NCCL INFO Channel 00 :    0   1\n[1,0]&lt;stdout&gt;:1203-050510-sloop247-10-139-64-5:8171:8181 [0] NCCL INFO Channel 01 :    0   1\n[1,0]&lt;stdout&gt;:1203-050510-sloop247-10-139-64-5:8171:8181 [0] NCCL INFO Ring 00 : 0[0] -&gt; 1[1] via direct shared memory\n[1,1]&lt;stdout&gt;:1203-050510-sloop247-10-139-64-5:8172:8179 [1] NCCL INFO Ring 00 : 1[1] -&gt; 0[0] via direct shared memory\n[1,0]&lt;stdout&gt;:1203-050510-sloop247-10-139-64-5:8171:8181 [0] NCCL INFO Ring 01 : 0[0] -&gt; 1[1] via direct shared memory\n[1,1]&lt;stdout&gt;:1203-050510-sloop247-10-139-64-5:8172:8179 [1] NCCL INFO Ring 01 : 1[1] -&gt; 0[0] via direct shared memory\n[1,0]&lt;stdout&gt;:1203-050510-sloop247-10-139-64-5:8171:8181 [0] NCCL INFO Using 128 threads, Min Comp Cap 3, Trees disabled\n[1,1]&lt;stdout&gt;:1203-050510-sloop247-10-139-64-5:8172:8179 [1] NCCL INFO comm 0x7f3a142d9840 rank 1 nranks 2 cudaDev 1 nvmlDev 1 - Init COMPLETE\n[1,0]&lt;stdout&gt;:1203-050510-sloop247-10-139-64-5:8171:8181 [0] NCCL INFO comm 0x7fbae02d9f30 rank 0 nranks 2 cudaDev 0 nvmlDev 0 - Init COMPLETE\n[1,0]&lt;stdout&gt;:1203-050510-sloop247-10-139-64-5:8171:8181 [0] NCCL INFO Launch mode Parallel\n[1,1]&lt;stdout&gt;:30000/30000 - 5s - loss: 1.0549 - acc: 0.6774 - val_loss: 0.3609 - val_acc: 0.8962\n[1,1]&lt;stdout&gt;:Epoch 2/5\n[1,0]&lt;stderr&gt;:2019-12-03 13:31:04.308029: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n[1,0]&lt;stdout&gt;:30000/30000 - 6s - loss: 1.1747 - acc: 0.6310 - val_loss: 0.3647 - val_acc: 0.8926\n[1,0]&lt;stdout&gt;:Epoch 2/5\n[1,1]&lt;stdout&gt;:30000/30000 - 4s - loss: 0.3774 - acc: 0.8860 - val_loss: 0.2604 - val_acc: 0.9246\n[1,1]&lt;stdout&gt;:Epoch 3/5\n[1,0]&lt;stdout&gt;:30000/30000 - 4s - loss: 0.5439 - acc: 0.8325 - val_loss: 0.2614 - val_acc: 0.9216\n[1,0]&lt;stdout&gt;:Epoch 3/5\n[1,1]&lt;stdout&gt;:30000/30000 - 4s - loss: 0.2861 - acc: 0.9158 - val_loss: 0.2095 - val_acc: 0.9354\n[1,1]&lt;stdout&gt;:Epoch 4/5\n[1,0]&lt;stdout&gt;:30000/30000 - 4s - loss: 0.4388 - acc: 0.8673 - val_loss: 0.2094 - val_acc: 0.9358\n[1,0]&lt;stdout&gt;:Epoch 4/5\n[1,1]&lt;stdout&gt;:30000/30000 - 4s - loss: 0.2345 - acc: 0.9289 - val_loss: 0.1897 - val_acc: 0.9404\n[1,1]&lt;stdout&gt;:Epoch 5/5\n[1,0]&lt;stdout&gt;:30000/30000 - 4s - loss: 0.3801 - acc: 0.8842 - val_loss: 0.1796 - val_acc: 0.9478\n[1,0]&lt;stdout&gt;:Epoch 5/5\n[1,1]&lt;stdout&gt;:30000/30000 - 4s - loss: 0.2073 - acc: 0.9387 - val_loss: 0.1542 - val_acc: 0.9518\n[1,0]&lt;stdout&gt;:30000/30000 - 4s - loss: 0.3407 - acc: 0.8972 - val_loss: 0.1548 - val_acc: 0.9522\n</div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["#### Veamos la performance del modelo en terminos de su loss function\n\nDurante la rutina de entrenamiento estamos guardando el historial donde registramos el loss como el accuracy tanto para validation como para training en cada uno de los epochs"],"metadata":{}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimg=mpimg.imread('/dbfs/ml/MNISTDemo/train/1575377203.3795166/train_acc.png')\nimgplot = plt.imshow(img)\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimg=mpimg.imread('/dbfs/ml/MNISTDemo/train/1575377203.3795166/train_loss.png')\nimgplot = plt.imshow(img)\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":22}],"metadata":{"name":"tensorflow-horovod","notebookId":1004252954942712},"nbformat":4,"nbformat_minor":0}
